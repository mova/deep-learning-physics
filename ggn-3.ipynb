{"cells":[{"cell_type":"markdown","metadata":{},"source":[" Adapted from https://github.com/DeepLearningForPhysicsResearchBook/deep-learning-physics/blob/main/Exercise_10_1.ipynb\n"," ## Task 3\n"," ## Signal Classification using Dynamic Graph Convolutional Neural Networks\n"," After a long journey through the universe before reaching the earth, the cosmic particles interact with the galactic magnetic field $B$.\n"," As these particles carry a charge $q$ they are deflected in the field by the Lorentz force $F = q \\cdot v Ã— B$.\n"," Sources of cosmic particles are located all over the sky, thus arrival distributions of the cosmic particles are isotropic in general. However, particles originating from the same source generate on top of the isotropic\n"," arrival directions, street-like patterns from galactic magnetic field deflections.\n","\n"," In this tasks we want to classify whether a simulated set of $500$ arriving cosmic particles contains street-like patterns (signal), or originates from an isotropic background.\n","\n"," Training graph networks can be computationally demanding, thus, we recommend to use a GPU for this task."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch_geometric.data import Data, Batch\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","from utils import CosmicRayDS\n","\n","ds = CosmicRayDS(\".\")\n","n_test = 10000\n","ds_train, ds_test = ds[:-n_test], ds[-n_test:]"]},{"cell_type":"markdown","metadata":{},"source":[" ## Task 3.1\n"," Extract a single event from the test dataset and inspect it.\n"," Plot an example sky map using the `skymap` function from `utils`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils import skymap\n","event0= ...\n","fig = skymap(..., c=event0.x, zlabel=\"Energy (normed)\", title=\"Event 0\")"]},{"cell_type":"markdown","metadata":{},"source":[" ## Task 3.2\n"," Generate edges for the event using `knn_graph`.\n"," Plot the edges by passing the `edge_index` to the `skymap` function. How does the number of edges scale with the $k$?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch_geometric.nn import knn_graph\n","\n","\n","fig = skymap(\n","    ...,\n","    c=...,\n","    edge_index=...,\n","    zlabel=\"Energy (normed)\",\n","    title=\"Event 0\",\n",")\n","# -> k*num_nodes"]},{"cell_type":"markdown","metadata":{},"source":["## Task 3.3\n","Write a class to return a simple Feed-Forward-Network (FFN) for a given number inputs and outputs. (3 layers, 20 hidden nodes, BatchNorm, LeakyReLU)\n","The final layer has neither activation nor norm."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class FFN(nn.Module):\n","    def __init__(self, n_in, n_out, n_hidden=20):\n","        super().__init__()\n","        self.seq = nn.Sequential(\n","            ...\n","        )\n","\n","    def forward(self, *args, **kwargs):\n","        return self.seq(*args, **kwargs)"]},{"cell_type":"markdown","metadata":{},"source":[" ## Task 3.4\n"," GNNs classifiers are frequently build in a two step process: First MessagePassingLayers( aka Graph [Convolutional Layers](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers) ) update the nodes. These exploit the local information. Then, the nodes are aggregated using [Pooling Layers](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#pooling-layers), reducing the graph to a single feature vector. This feature vector is then passed through a FFN to get the classification output.\n"," Have a look at the documentation of [EdgeConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.EdgeConv) and [DynamicEdgeConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.DynamicEdgeConv).\n"," What is the difference?\n"," -> `EdgeConv` requires a `edge_index` while `DynamicEdgeConv` constructs the `edge_index` on the feature space.\n"," What the input space of the `nn` passed to EdgeConv?\n"," -> 2* num_features\n"," Implement a GNN class with three MPL (not MLP!) using EdgeConv\n"," and DynamicEdgeConv. For the first MPL, we want to construct\n"," the `edge_index` on the feature space.\n"," Use both the energies of the particles (`batch.x`) as well as their positions (`batch.pos`) as an input to the first MPL.\n"," For the other two layer we may (or may not) choose to construct the `edge_index` on the feature space.\n"," > Sidenote: Running `knn` multiple times per forward-pass might be quite expensive, depending on the number of nodes and the dimensionality of the space.\n"," After the MPLs apply a `global_X_pool` and pass the result through a FFN projecting to a single node."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch_geometric.nn import knn_graph, EdgeConv, DynamicEdgeConv, global_add_pool\n","\n","\n","class GNN(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.conv1 = ...\n","\n","    def forward(self, batch: Batch):\n","        # We run knn on the positions\n","        # knn needs to know about the batches, otherwise it connects\n","        # points from different events\n","        edge_index = ...\n","        x = torch.hstack([batch.x, batch.pos])\n","        ...\n","        return x.squeeze()"]},{"cell_type":"markdown","metadata":{},"source":[" ## Task 3.5\n"," Fill in  the gaps to implement a training loop.\n"," > The [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) is recommended as it combines a Sigmoid layer and the `BCELoss`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch_geometric.loader import DataLoader\n","from utils import metric_aggr\n","\n","device = torch.device(\"cuda\")\n","loader = DataLoader(ds_train, batch_size=64, shuffle=True)\n","model = GNN().to(device)\n","optim = ...\n","loss_f = nn.BCEWithLogitsLoss()\n","\n","for iepoch in range(?):\n","    for batch in tqdm(loader):\n","        ..."]},{"cell_type":"markdown","metadata":{},"source":[" ## Task 3.6\n"," Collect the outputs for the test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loader = DataLoader(ds_test, batch_size=6, shuffle=True)\n","model.eval()\n","output_list = []\n","with torch.no_grad():\n","    for batch in tqdm(loader):\n","        ...\n","ytrue, yhat = torch.hstack(output_list).cpu().numpy()"]},{"cell_type":"markdown","metadata":{},"source":[" ## Task 3.7\n"," Evalutate the model performance on the test set by computing the AUC."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","..."]},{"cell_type":"markdown","metadata":{},"source":[" ## Task 3.8 - Bonus/Open end\n"," Optimize the model for AUC and speed."]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.6 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"cf1e7457895ee7dafed1f4a11df8662f6860406cb300c73c45b696a5411f164d"}}},"nbformat":4,"nbformat_minor":2}
